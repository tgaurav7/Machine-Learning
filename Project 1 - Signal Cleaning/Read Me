The folder contains four projects that uses Keras for implementing supervised learning using different Machine Learning models:
  1. Signal Cleaning: Predict the signal from background noise using neural networks. 
      This code models a neural network with different parameters to model prediction of 'signal' or 'background data based on 27 f values for each case. The data is read as dataframe 'data' and split into training and testing data(80:20) using the train_test_split library from model_selection. 3 initial tests were performed to see the effect of number of nodes in the hidden layer(numnodes) namely for 10, 50 and 100 with sigmoid-activation, sgd-optimization and mean_squared_test-loss functions without any regularization, dropout and batchnormalization. Since, 10 nodes gave the best result with test data among the three and was fastest, the remaining tests were performed with 10 nodes. These include: 1. With regularization (lam = 0.001) 2. With dropouts (d_rate = 0.2) 3. With batchnormalization 4. In addition to batchnormalization, with 'tanh' as hidden layer activation function and 'softmax' for output layer 5. In addition to above, the 'adam' optimization algorithm and 'categorical_crossentropy' loss function The results for the tests are given as(score[2]): Running simple sigmoid function and Sgd optimization - numnodes 10 0.815250 numnodes 50 0.814300 numnodes 100 0.814175 Running with regularizationregularization_nunodes10_epochs_500_batchsize_5000 0.813850 Running with dropouts 0.812200 Running with updated activation functions 0.814525 Running with batchnormalization and updated activation functions 0.816050 Running with batchnormalization, updated activation, optimization and loss functions 0.817125 The best results were with 10 nodes was with including all the varied parameters i.e. the last model. However, the model with 50 nodes was also trained and tested. The results with 50 numnodes are given as: Running simple sigmoid function and Sgd optimization - numnodes 10 0.819600 numnodes 50 0.819250 numnodes 100 0.818875 Running with regularizationregularization_nunodes50_epochs_500_batchsize_5000 0.819175 Running with dropouts, drop rate = 40% 0.819075 Running with updated activation functions 0.820625 Running with batchnormalization and updated activation functions 0.819575 Running with batchnormalization, updated activation, optimization and loss functions 0.818075 As seen, the results with 50 nodes with the time spent considerably higher are slightly better (3rd order of decimal only) with the best of 0.819600 with the initial case i.e. sigmoid functions for hidden and output layers and no other parameters.
