The folder contains four projects that uses Keras for implementing supervised learning using different Machine Learning models:
  1. Signal Cleaning: Predict the signal from background noise using neural networks. 
      This code models a neural network with different parameters to model prediction of 'signal' or 'background data based on 27 f values for each case. The data is read as dataframe 'data' and split into training and testing data(80:20) using the train_test_split library from model_selection. 3 initial tests were performed to see the effect of number of nodes in the hidden layer(numnodes) namely for 10, 50 and 100 with sigmoid-activation, sgd-optimization and mean_squared_test-loss functions without any regularization, dropout and batchnormalization. Since, 10 nodes gave the best result with test data among the three and was fastest, the remaining tests were performed with 10 nodes. These include: 1. With regularization (lam = 0.001) 2. With dropouts (d_rate = 0.2) 3. With batchnormalization 4. In addition to batchnormalization, with 'tanh' as hidden layer activation function and 'softmax' for output layer 5. In addition to above, the 'adam' optimization algorithm and 'categorical_crossentropy' loss function The results for the tests are given as(score[2]): Running simple sigmoid function and Sgd optimization - numnodes 10 0.815250 numnodes 50 0.814300 numnodes 100 0.814175 Running with regularizationregularization_nunodes10_epochs_500_batchsize_5000 0.813850 Running with dropouts 0.812200 Running with updated activation functions 0.814525 Running with batchnormalization and updated activation functions 0.816050 Running with batchnormalization, updated activation, optimization and loss functions 0.817125 The best results were with 10 nodes was with including all the varied parameters i.e. the last model. However, the model with 50 nodes was also trained and tested. The results with 50 numnodes are given as: Running simple sigmoid function and Sgd optimization - numnodes 10 0.819600 numnodes 50 0.819250 numnodes 100 0.818875 Running with regularizationregularization_nunodes50_epochs_500_batchsize_5000 0.819175 Running with dropouts, drop rate = 40% 0.819075 Running with updated activation functions 0.820625 Running with batchnormalization and updated activation functions 0.819575 Running with batchnormalization, updated activation, optimization and loss functions 0.818075 As seen, the results with 50 nodes with the time spent considerably higher are slightly better (3rd order of decimal only) with the best of 0.819600 with the initial case i.e. sigmoid functions for hidden and output layers and no other parameters.
  2. Outlier Detection: Detect malicious login attempts using autoencoders: 
      The code contained reads the dataset of predetermined normal and malicious login attempts and scales each of the features using minmax function from SciKit Learn in range [0, 1]. The autoencoder built using Keras is trained and tested with the divided dataset to set a threshold between normal and malicious login attempts. The code uses varying parameters in the model(number of hidden layers, number of nodes per layer, activation functions, cost function, optimization algorithm and the impact of regularization, batch normalization and dropout) for the model to find the best confusion matrix, and thus the best separation threshold. It is suggested that the cross-entropy loss function in the present version be replaced with MSE. 
  3. Motion Detection from Time-Series data: The current code reads the dataset of time-series sensor data by people who are performing different actions and the aims at creating LSTM to classify motion detected by the sensors. 
  4. Number Recognition: This project aims at using Varaitional Autoencoders for predicting the numbers from MNIST dataset. 
  CNNs have been used to perform the Image Classification using the ILSVRC challenge dataset (https://image-net.org/challenges/LSVRC/). Some well known solutions for the problem are given in the resources: 
          a. AlexNet: http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf
          b. ResNet: https://arxiv.org/abs/1512.03385
          c. GoogLeNet: https://arxiv.org/abs/1409.4842
          d. Xception: https://arxiv.org/abs/1610.02357
